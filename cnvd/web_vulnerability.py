# -*- coding:utf-8 -*-

import re
from random import sample
from time import sleep
import requests
from bs4 import BeautifulSoup
import os
import time
import MySQLdb
#import Fetch_proxy
import Parse_config
from requests.adapters import HTTPAdapter
from splinter import Browser

sleep_num = 15
requests.adapters.DEFAULT_RETRIES = 5

config_f='config.ini'
realdir=os.path.split(os.path.realpath(__file__))[0]
cf = Parse_config.config_parse(os.path.join(realdir,config_f))
dbhost=cf.get_dbhost()
dbuser=cf.get_dbuser()
dbpwd=cf.get_dbpwd()
dbname=cf.get_dbname()

pagenum=cf.get_page_num()


__user_agent = [
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.5 (KHTML, like Gecko) Chrome/4.0.249.0 Safari/532.5",
    "Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/532.9 (KHTML, like Gecko) Chrome/5.0.310.0 Safari/532.9",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.7 (KHTML, like Gecko) Chrome/7.0.514.0 Safari/534.7",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/9.0.601.0 Safari/534.14",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.601.0 Safari/534.14",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.27 (KHTML, like Gecko) Chrome/12.0.712.0 Safari/534.27",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.24 Safari/535.1",
    "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.874.120 Safari/535.2",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0 x64; en-US; rv:1.9pre) Gecko/2008072421 Minefield/3.0.2pre",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.10) Gecko/2009042316 Firefox/3.0.10",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-GB; rv:1.9.0.11) Gecko/2009060215 Firefox/3.0.11 (.NET CLR 3.5.30729)",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6 GTB5",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; tr; rv:1.9.2.8) Gecko/20100722 Firefox/3.6.8 ( .NET CLR 3.5.30729; .NET4.0E)",
    "Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",
    "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",
    "Mozilla/5.0 (Windows NT 5.1; rv:5.0) Gecko/20100101 Firefox/5.0",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0a2) Gecko/20110622 Firefox/6.0a2",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:7.0.1) Gecko/20100101 Firefox/7.0.1",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:2.0b4pre) Gecko/20100815 Minefield/4.0b4pre",
    "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT 5.0 )",
    "Mozilla/4.0 (compatible; MSIE 5.5; Windows 98; Win 9x 4.90)",
    "Mozilla/5.0 (Windows; U; Windows XP) Gecko MultiZilla/1.6.1.0a",
    "Mozilla/2.02E (Win95; U)",
    "Mozilla/3.01Gold (Win95; I)",
    "Mozilla/4.8 [en] (Windows NT 5.1; U)",
    "Avant Browser/1.2.789rel1 (http://www.avantbrowser.com)",
    "Mozilla/5.0 (Windows; U; Win98; en-US; rv:1.4) Gecko Netscape/7.1 (ax)",
    "Mozilla/5.0 (Linux; U; Android 0.5; en-us) AppleWebKit/522  (KHTML, like Gecko) Safari/419.3",
    "HTC_Dream Mozilla/5.0 (Linux; U; Android 1.5; en-ca; Build/CUPCAKE) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (hp-tablet; Linux; hpwOS/3.0.2; U; de-DE) AppleWebKit/534.6 (KHTML, like Gecko) wOSBrowser/234.40.1 Safari/534.6 TouchPad/1.0",
    "Mozilla/5.0 (Linux; U; Android 1.5; en-us; sdk Build/CUPCAKE) AppleWebkit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 2.1; en-us; Nexus One Build/ERD62) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 1.5; en-us; htc_bahamas Build/CRB17) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 2.1-update1; de-de; HTC Desire 1.19.161.5 Build/ERE27) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; Sprint APA9292KT Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 1.5; de-ch; HTC Hero Build/CUPCAKE) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; ADR6300 Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 2.1; en-us; HTC Legend Build/cupcake) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 1.5; de-de; HTC Magic Build/PLAT-RC33) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1 FirePHP/0.3",
    "Mozilla/5.0 (Linux; U; Android 1.6; en-us; HTC_TATTOO_A3288 Build/DRC79) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 1.0; en-us; dream) AppleWebKit/525.10  (KHTML, like Gecko) Version/3.0.4 Mobile Safari/523.12.2",
    "Mozilla/5.0 (Linux; U; Android 1.5; en-us; T-Mobile G1 Build/CRB43) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari 525.20.1",
    "Mozilla/5.0 (Linux; U; Android 1.5; en-gb; T-Mobile_G2_Touch Build/CUPCAKE) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 2.0; en-us; Droid Build/ESD20) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; Droid Build/FRG22D) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 2.0; en-us; Milestone Build/ SHOLS_U2_01.03.1) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.0.1; de-de; Milestone Build/SHOLS_U2_01.14.0) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 3.0; en-us; Xoom Build/HRI39) AppleWebKit/525.10  (KHTML, like Gecko) Version/3.0.4 Mobile Safari/523.12.2",
    "Mozilla/5.0 (Linux; U; Android 1.1; en-gb; dream) AppleWebKit/525.10  (KHTML, like Gecko) Version/3.0.4 Mobile Safari/523.12.2",
    "Mozilla/5.0 (Linux; U; Android 2.0; en-us; Droid Build/ESD20) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.1; en-us; Nexus One Build/ERD62) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; Sprint APA9292KT Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-us; ADR6300 Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 2.2; en-ca; GT-P1000M Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 2.3.6; en-us; Nexus S Build/GRK39F) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Linux; U; Android 3.0.1; fr-fr; A500 Build/HRI66) AppleWebKit/534.13 (KHTML, like Gecko) Version/4.0 Safari/534.13",
    "Mozilla/5.0 (Linux; U; Android 3.0; en-us; Xoom Build/HRI39) AppleWebKit/525.10  (KHTML, like Gecko) Version/3.0.4 Mobile Safari/523.12.2",
    "Mozilla/5.0 (Linux; U; Android 1.6; es-es; SonyEricssonX10i Build/R1FA016) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
    "Mozilla/5.0 (Linux; U; Android 1.6; en-us; SonyEricssonX10i Build/R1AA056) AppleWebKit/528.5  (KHTML, like Gecko) Version/3.1.2 Mobile Safari/525.20.1",
]


def get_ua():
    return sample(__user_agent, 1)[0]


def init_headers():
    headers = { "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                     "Accept-Encoding":"gzip, deflate, sdch",
                     "Accept-Language":"zh-CN,zh;q=0.8",
                     "Referer":"http://www.cnvd.org.cn/flaw/typelist?typeId=29",
                     "User-Agent":get_ua(),
                     "Cache-Control":"max-age=0",
                     "Host":"www.cnvd.org.cn",
                     "Connection":"keep-alive",
                     "Upgrade-Insecure-Requests":"1",
                     "X-Forwarded-For":""
                     }
    return headers

def get_cookie(url,ua):
    browser = Browser('chrome', headless=True, user_agent=ua)
    browser.visit(url)
    sleep(3)
    tmp = browser.cookies.all()

    cookie=''
    for key in tmp:
        cookie += str(key) +'=' + tmp[key] +';'
        
    browser.quit()
    return cookie

    
class Professor():
    def __init__(self,pagenum):
        self.db = MySQLdb.connect(dbhost,dbuser,dbpwd,dbname,charset='utf8')
        self.cursor = self.db.cursor()
        
        self.url = 'http://www.cnvd.org.cn/flaw/typeResult?max=20&offset=' + str(20*pagenum) + '&typeId=29'
        self.ua = get_ua()
        self.headers = { "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                     "Accept-Encoding":"gzip, deflate, sdch",
                     "Accept-Language":"zh-CN,zh;q=0.8",
                     "Referer":"http://www.cnvd.org.cn/flaw/typelist?typeId=29",
                     "User-Agent":self.ua,
                     "Host":"www.cnvd.org.cn",
                     "Connection":"keep-alive",
                     "Upgrade-Insecure-Requests":"1",
                     "Cookie":""
                     }
        
    def open_url_with_soup(self,url):        
        #sleep(sleep_num)
        
        #proxy_get=Fetch_proxy.Fetch_proxy()
        while True:
            #proxy_ip=proxy_get.fetch()
            self.seq = requests.Session()
            '''
            self.proxies={
            'http':'http://%s'%proxy_ip
            }
            '''
            self.seq.mount('http://', HTTPAdapter(max_retries=5))
            try:
                self.headers["User-Agent"] = get_ua()
                self.headers["Cookie"] = get_cookie(url,self.headers["User-Agent"])
                res=requests.get(url,headers=self.headers,timeout=5)
                res.encoding = 'utf-8'
                if res.status_code == 200:
                    break
            except requests.exceptions.HTTPError as e:
                #print 'proxy is expiered ',proxy_ip
                #proxy_get.delete(proxy_ip)
                print str(e)
                exit
            except requests.exceptions.ConnectionError as e:
                #print 'proxy is expiered ',proxy_ip
                #proxy_get.delete(proxy_ip)
                print str(e)
                exit 
            except:
                #print 'proxy is expiered ',proxy_ip
                #proxy_get.delete(proxy_ip)
                exit 
                
        if res.status_code == 200:
            html = BeautifulSoup(res.text,'html5lib')
            print "open url "+str(url)+ "  success "
            if html.find_all(name="table", attrs={"class":"tlist"}) or html.findAll(name="table",attrs={"class":"gg_detail"}) :
                return html
            else:
                return self.open_url_with_soup(url)
        else:
            print "open url "+str(url)+ "  fail "

            
    def get_url_list(self):
        try:
            html = self.open_url_with_soup(self.url)
            table = html.findAll(name="table",attrs={"class":"tlist"})
            self.get_vul_info(table)
        except Exception,e:
            print str(e)
            pass
        
    def get_vul_info(self,table):
        vul = table[0].findAll('tr')
        for i in vul:
            vulinfo = i.findAll('td')
            title = vulinfo[0].text.encode('utf-8').strip()
            get_href = vulinfo[0].findAll('a')
            href = get_href[0].get('href')

            self.get_content(title,"http://www.cnvd.org.cn" + href)

                
    def get_content(self,title,url):
        
        tag = self.check_title(title)
        if tag == 1:
            print title," already exists!"
            return 1
        print title," not exists!"
    
        cnvdid = ''
        publicdate = ''
        vullevel = ''
        impactproducts = ''
        cveid = ''
        description = ''
        reference = ''
        solutions = ''
        path = ''
        verification = ''
        insert_array=[]

        try:
            html = self.open_url_with_soup(url)
            content = html.findAll(name="table",attrs={"class":"gg_detail"})
            trs = content[0].findAll('tr')
            for tr in trs:
                tmp = tr.text.encode('utf-8').strip()
                if tmp.startswith('CNVD-ID'):
                    try:
                        cnvdid_tmp = tr.findAll('td')
                        cnvdid = cnvdid_tmp[1].text.encode('utf-8').strip()
                        #print cnvdid
                    except:
                        pass
                    
                elif tmp.startswith('公开日期'):
                    try:
                        publicdate_tmp = tr.findAll('td')
                        publicdate = publicdate_tmp[1].text.encode('utf-8').strip()
                        #print publicdate
                    except:
                        pass
                    
                elif tmp.startswith('危害级别'):
                    try:
                        vullevel_tmp = tr.findAll('td')
                        vullevel = re.search(r'(高|中|低)', vullevel_tmp[1].text.encode('utf-8').strip(), re.S).group(1)
                        #print vullevel
                    except:
                        pass
                    
                elif tmp.startswith('影响产品'):
                    try:
                        impactproducts_tmp = tr.findAll('td')
                        impactproducts = impactproducts_tmp[1].text.encode('utf-8').strip()
                        impactproducts = re.sub(r"\s{2,}","\n",impactproducts)
                        #print impactproducts
                    except:
                        pass
                    
                elif tmp.startswith('CVE'):
                    try:
                        cveid_tmp = tr.findAll('td')
                        cveid = cveid_tmp[1].text.encode('utf-8').strip()
                        #print cveid
                    except:
                        pass
                
                elif tmp.startswith('漏洞描述'):
                    try:
                        description_tmp = tr.findAll('td')
                        description = description_tmp[1].text.encode('utf-8').strip()
                        description = re.sub(r"\s+"," ",description)
                        #print description
                    except:
                        pass
                    
                elif tmp.startswith('参考链接'):
                    try:
                        reference_tmp = tr.findAll('td')
                        reference = reference_tmp[1].text.encode('utf-8').strip()
                        #print reference
                    except:
                        pass
                    
                elif tmp.startswith('漏洞解决方案'):
                    try:
                        solutions_tmp = tr.findAll('td')
                        solutions = solutions_tmp[1].text.encode('utf-8').strip()
                        solutions = re.sub(r"\s+"," ",solutions)
                        #print solutions
                    except:
                        pass
                    
                elif tmp.startswith('厂商补丁'):
                    try:
                        path_tmp = tr.findAll('td')
                        path = path_tmp[1].text.encode('utf-8').strip()
                        #print path
                    except:
                        pass
                    
                elif tmp.startswith('验证信息'):
                    try:
                        verification_tmp = tr.findAll('td')
                        verification = verification_tmp[1].text.encode('utf-8').strip()
                        #print verification
                    except:
                        pass


        except Exception,e:
            print str(e)
            pass
        
        insert_array.append(title)   
        insert_array.append(cnvdid)    
        insert_array.append(publicdate)
        insert_array.append(vullevel)
        insert_array.append(impactproducts)
        insert_array.append(cveid)
        insert_array.append(description.replace('\'','\\\''))
        insert_array.append(reference.replace('\'','\\\''))
        insert_array.append(solutions.replace('\'','\\\''))
        insert_array.append(path)
        insert_array.append(verification)
        insert_array.append(1)
        self.insert_into_table(insert_array)
        
    
    def check_title(self,title):
        sql="select * from t_cnvd_vul where title='%s'" % (title)
        total = self.cursor.execute(sql)
        tag = 0;
        if total == 0:
            return tag
        else:
            tag = 1
        return tag        
    
    def insert_into_table(self,insert_list):
        sql="insert into t_cnvd_vul(title,cnvdid,publicdate,vullevel,impactproducts,cveid,description,reference,solutions,path,verification,classification) values('{0}','{1}','{2}','{3}','{4}','{5}','{6}','{7}','{8}','{9}','{10}','{11}')"  .format(insert_list[0],insert_list[1],insert_list[2],insert_list[3],insert_list[4],insert_list[5],insert_list[6],insert_list[7],insert_list[8],insert_list[9],insert_list[10],insert_list[11])
        try:
            self.cursor.execute(sql)
            self.db.commit()
            self.write_record(insert_list[0]+" insert success")
            print insert_list[0],"insert success"
        except Exception,e:
            self.write_record(insert_list[0]+" insert error")
            print insert_list[0],"insert error"
            print str(e)
            self.db.rollback()

    
    def write_record(self,tmp):
        fp = open('spider.log','a')
        ISOTIMEFORMAT='%Y-%m-%d %X'
        tt = time.strftime( ISOTIMEFORMAT, time.localtime() )
        fp.write(tt+' '+ tmp+'\n')
        fp.close()



if __name__=="__main__":
    
    for i in range(0,pagenum):
        p = Professor(i)
        p.get_url_list()
        
        
        
